{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/open-mmlab/mmediting/blob/master/demo/matting_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9VoEGTL_R50"
   },
   "source": [
    "# MMEditing Tutorial-Matting\n",
    "\n",
    "Welcome to MMEditing! This is the official colab tutorial for using MMEditing for matting task. In this tutorial, you will learn to\n",
    "\n",
    "* perform inference with a MMEditing mattor,\n",
    "* train a new mattor with a new dataset.\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-R6v39d__uJl"
   },
   "source": [
    "## Install MMEditing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch version\n",
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mmcv-full dependency via openmim\n",
    "!pip install openmim\n",
    "!mim install mmcv-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mmediting from source\n",
    "!git clone https://github.com/open-mmlab/mmediting.git\n",
    "%cd mmediting\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MMEditing installation\n",
    "import mmedit\n",
    "print(mmedit.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-3YqEpeF9R6"
   },
   "source": [
    "## Perform inference with a MMEditing mattor\n",
    "\n",
    "MMEditing already provides high level APIs to do inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available IndexNet models\n",
    "!mim search mmedit --model indexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download config and checkpoints for IndexNet to current working directory\n",
    "!mkdir -p checkpoints\n",
    "!mim download mmedit --config indexnet_mobv2_1x16_78k_comp1k --dest checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmedit.apis import matting_inference, init_model\n",
    "\n",
    "# Choose to use a config and initialize the mattor\n",
    "config = 'configs/mattors/indexnet/indexnet_mobv2_1x16_78k_comp1k.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = 'checkpoints/indexnet_mobv2_1x16_78k_comp1k_SAD-45.6_20200618_173817-26dd258d.pth'\n",
    "# Initialize the recognizer\n",
    "model = init_model(config, checkpoint, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mmcv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Sample images path\n",
    "merged_path = './tests/data/merged/GT05.jpg'\n",
    "trimap_path = './tests/data/trimap/GT05.png'\n",
    "\n",
    "# Plot sample images\n",
    "merged = mmcv.imread(merged_path)\n",
    "trimap = mmcv.imread(trimap_path)\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "f.dpi = 160\n",
    "axarr[0].axis('off')\n",
    "axarr[0].imshow(mmcv.bgr2rgb(merged))\n",
    "axarr[1].axis('off')\n",
    "axarr[1].imshow(trimap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the mattor to do inference\n",
    "pred_alpha = matting_inference(model, merged_path, trimap_path) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "plt.gcf().dpi = 80\n",
    "plt.axis('off')\n",
    "plt.imshow(pred_alpha, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2s8h_rqRX-3"
   },
   "source": [
    "## Train a mattor on a customized dataset\n",
    "\n",
    "To train a new mattor, there are usually three things to do:\n",
    "\n",
    "1. Support a new dataset\n",
    "2. Modify the config\n",
    "3. Train a new mattor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gxs3HR86RatF"
   },
   "source": [
    "### Support a new dataset\n",
    "\n",
    "In this tutorial, we gives an example to convert the data into the format of existing datasets. \n",
    "\n",
    "<!-- TODO: Other methods and more advanced usages can be found in the [doc](TODO). -->\n",
    "\n",
    "Firstly, let's download the only available open source matting dataset from [alphamatting.com](http://alphamatting.com/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/alphamatting/\n",
    "!curl http://alphamatting.com/datasets/zip/input_training_lowres.zip -o data/alphamatting/input_training_lowres.zip\n",
    "!curl http://alphamatting.com/datasets/zip/trimap_training_lowres.zip -o data/alphamatting/trimap_training_lowres.zip\n",
    "!curl http://alphamatting.com/datasets/zip/gt_training_lowres.zip -o data/alphamatting/gt_training_lowres.zip\n",
    "!unzip -o data/alphamatting/input_training_lowres.zip -d data/alphamatting/merged\n",
    "!unzip -o data/alphamatting/trimap_training_lowres.zip -d data/alphamatting/trimap\n",
    "!unzip -o data/alphamatting/gt_training_lowres.zip -d data/alphamatting/alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkmZzafQWrLP"
   },
   "source": [
    "Then we create the annotation file for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_file = 27\n",
    "num_training = 20  # use 20 samples for training, 7 for test\n",
    "\n",
    "training_ann = list()\n",
    "for i in range(num_training):\n",
    "  ann = dict()\n",
    "  ann['merged_path'] = f'merged/GT{i+1:02d}.png'\n",
    "  ann['alpha_path'] = f'alpha/GT{i+1:02d}.png'\n",
    "  # since data from alphamatting.com is not composited, we use original image \n",
    "  # as fg and bg\n",
    "  ann['fg_path'] = ann['merged_path']\n",
    "  ann['bg_path'] = ann['merged_path']\n",
    "  training_ann.append(ann)\n",
    "\n",
    "import mmcv\n",
    "mmcv.dump(training_ann, './data/alphamatting/training_list.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKWmqxkdZA51"
   },
   "source": [
    "Let's create the annotation file for test data in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trimap = 1\n",
    "test_ann = list()\n",
    "for i in range(num_training, num_file):\n",
    "  for j in range(num_trimap):\n",
    "    ann = dict()\n",
    "    ann['merged_path'] = f'merged/GT{i+1:02d}.png'\n",
    "    ann['trimap_path'] = f'trimap/Trimap{j+1}/GT{i+1:02d}.png'\n",
    "    ann['alpha_path'] = f'alpha/GT{i+1:02d}.png'\n",
    "    test_ann.append(ann)\n",
    "\n",
    "mmcv.dump(test_ann, './data/alphamatting/test_list.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URUDy9x8cs-U"
   },
   "source": [
    "### Modify the config\n",
    "\n",
    "In the next step, we need to modify the config for the training. To accelerate the process, we finetune a mattor using a pre-trained mattor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/mattors/indexnet/indexnet_mobv2_1x16_78k_comp1k.py')\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P04K_cv-dIew"
   },
   "source": [
    "Given a config that trains a IndexNet model on Adobe Composition-1k dataset, we need to modify some values to use it for training IndexNet on the dataset we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('./configs/mattors/indexnet/indexnet_mobv2_1x16_78k_comp1k.py')\n",
    "\n",
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.data.train.type = 'AdobeComp1kDataset'\n",
    "cfg.data.train.ann_file = './data/alphamatting/training_list.json'\n",
    "cfg.data.train.data_prefix = './data/alphamatting/'\n",
    "\n",
    "cfg.data.val.type = 'AdobeComp1kDataset'\n",
    "cfg.data.val.ann_file = './data/alphamatting/test_list.json'\n",
    "cfg.data.val.data_prefix = './data/alphamatting/'\n",
    "\n",
    "cfg.data.test.type = 'AdobeComp1kDataset'\n",
    "cfg.data.test.ann_file = './data/alphamatting/test_list.json'\n",
    "cfg.data.test.data_prefix = './data/alphamatting/'\n",
    "\n",
    "# We can use the pre-trained IndexNet model\n",
    "cfg.model.pretrained = None\n",
    "cfg.load_from = './checkpoints/indexnet_mobv2_1x16_78k_comp1k_SAD-45.6_20200618_173817-26dd258d.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './tutorial_exps/indexnet'\n",
    "\n",
    "# Use smaller batch size for training\n",
    "cfg.data.train_dataloader.samples_per_gpu = 12\n",
    "cfg.data.workers_per_gpu = 1\n",
    "\n",
    "# The original learning rate (LR) is set for batch size 16 with 1 GPU.\n",
    "# We reduce the lr by a factor of 4 since we reduce the batch size.\n",
    "cfg.optimizers.lr = cfg.optimizers.lr / 4\n",
    "cfg.total_iters = 50\n",
    "cfg.lr_config = None\n",
    "\n",
    "# Evaluate every 20 iterations\n",
    "cfg.evaluation.interval = 20\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 40\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 5\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpus = 1\n",
    "\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI3uX5RqhFQQ"
   },
   "source": [
    "### Train a new mattor\n",
    "\n",
    "Finally, lets initialize the dataset and mattor, then train a new mattor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmedit.datasets import build_dataset\n",
    "from mmedit.models import build_model\n",
    "from mmedit.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the mattor\n",
    "model = build_model(cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "\n",
    "# train the model\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPRUWpQ1Dy8W"
   },
   "source": [
    "### Understand the log\n",
    "\n",
    "From the log, we can have a basic understanding the training process and know how well the mattor is trained.\n",
    "\n",
    "Firstly, the MobileNetV2 backbone pre-trained on ImageNet is loaded, this is a common practice since training from scratch is more costly. The log shows that all the weights of the MobileNetV2 backbone are loaded while some parts of the model are missing in the loaded weight. But no worries! These parts have a larger learning rate than the pretrained parts (You can print out `cfg.optimizers` to see the pretrained parts with name prefix `encoder.layers` have a `lr_mult=0.01`).\n",
    "\n",
    "Secondly, after training, the mattor is evaluated by the default evaluation. The results show that the mattor achieves lower and lower SAD along the training process,\n",
    "\n",
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R73szzAYL4cu"
   },
   "source": [
    "## Test the trained mattor\n",
    "\n",
    "After finetuning the recognizer, let's check the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmedit.apis import single_gpu_test\n",
    "from mmedit.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "\n",
    "# Build a test dataloader and model\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=1,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "\n",
    "# Perform the test with single gpu. Saving result images by setting\n",
    "# save_image and save_path arguments. The two arguments will be passed\n",
    "# to model.forword_test() where images are saved.\n",
    "# See https://github.com/open-mmlab/mmediting/blob/8b5c0c5f49e60fd6ab0503031b62dee7832faf72/mmedit/models/mattors/indexnet.py#L72.\n",
    "outputs = single_gpu_test(model, data_loader, save_image=True, save_path='./tutorial_exps/indexnet/results')\n",
    "\n",
    "# Pop out some unnecessary arguments\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_config.pop('save_image', False)\n",
    "eval_config.pop('save_path', None)\n",
    "\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "print()  # endline of progress bar\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGhi85oLtyvj"
   },
   "source": [
    "The result is the same as the last validation! This is because we use the same data for test and validation. Next, let's take a look at the visual results of the test! The results are saved in the directory `tutorial_exps/indexnet/results` that we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls tutorial_exps/indexnet/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8FO7XljukjJ"
   },
   "source": [
    "Plot the sample `GT21`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images path\n",
    "merged_path = './data/alphamatting/merged/GT21.png'\n",
    "trimap_path = './data/alphamatting/trimap/Trimap1/GT21.png'\n",
    "alpha_path = './tutorial_exps/indexnet/results/GT21.png'\n",
    "\n",
    "# Plot sample images\n",
    "merged = mmcv.imread(merged_path)\n",
    "trimap = mmcv.imread(trimap_path)\n",
    "alpha = mmcv.imread(alpha_path)\n",
    "f, axarr = plt.subplots(1, 3)\n",
    "f.dpi = 240\n",
    "axarr[0].axis('off')\n",
    "axarr[0].imshow(mmcv.bgr2rgb(merged))\n",
    "axarr[1].axis('off')\n",
    "axarr[1].imshow(trimap)\n",
    "axarr[2].axis('off')\n",
    "axarr[2].imshow(alpha)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmYZRxw6wefo"
   },
   "source": [
    "Congratulations! You've done the tutorial of using MMEditing for matting task. Go to [Getting Started page](https://github.com/open-mmlab/mmediting/blob/master/docs/en/getting_started.md) for more usage of MMEditing and start to train your own mattor!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e50b30ef82b36a0cbbf6f21fd772be4a0a3d581b2941b9ee661a026775044f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mmcv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
